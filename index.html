<!DOCTYPE html>
<html>
<head>
    <title>个人介绍</title>
    <style>
        div {
        margin-left: 15%;
        margin-right: 15%;
        /*width: 210px;
        height: 130px;
         background:green; */
        border-radius: 13px;
        }
        .sample {
        background-color: green;
        margin-left: 3%;
        }
        .sample1 {
        background-color: yelllow;
        }
        .sample2 {
        background-color: solid green;
        margin-left: -13%;
        }
        .photo{
            /*width:120px;
            height: auto;*/
            margin-right: 20%;
            float: right;
            text-align: right;
        }
        </style>
</head>
<body>
    <div class="photo">
        <img src="./cv.jpg" alt="Personal photo", width=120px,  height=auto>
    </div>
<div>
    <h1>张裕浩</h1>
    <h2>自然语言处理实验室 东北大学 沈阳</h2>
    <p>机器翻译方向 博士三年级 </br> Email: yoohao.zhang@gmail.com</p>
    <p>主要研究方向：<b>1.语音翻译，2.多任务学习，3.多语言神经机器翻译，4.模型加速。</b> 模型加速主要分为针对于底层张量并行计算库进行加速以及模型去冗余计算两个方向；多语言神经翻译方向主要利用一个模型翻译多个任务。目前已将其运用在IWLST，WMT等多个评测任务中并取得优异成绩。</p>
    <h2>教育经历</h2>
    <dl>
        <dt>2020.9 至今 </dt>
        <dd>计算机科学与技术 博士研究生 东北大学 </dd>
        <dd>东北大学自然语言处理实验室 导师：肖桐教授，朱靖波教授</dd>
        <dt>2018.9-2020.7 </dt>
        <dd>计算机软件与理论 硕士研究生 东北大学</dd>
        <dd>东北大学自然语言处理实验室 导师：肖桐教授</dd>
        <dt>2014.9-2018.7  </dt>
        <dd>计算机科学与技术 本科 东北大学</dd>
      </dl>
 
</div>


<div>
    <h2>学习经历 </h2> 
    <b>面向语音识别的多尺度建模</b>                               2023.2-2023.5 </br>
    使用多尺度建模方式增强端到端语音识别编码，进而提升性能。 已被CCL2023接收 </br>
    <b>IWSLT2023评测任务</b>                               2023.2-2023.4 </br>
    参与IWSLT2023 离线英中端到端语音翻译比赛，主要运用了多任务学习策略、多轮伪数据方法和基于预训练的音频切分方法。 </br>
    <b>动态语音特征压缩</b>                          2022.5-2023.1 </br>
    语音前端的特征压缩方法探索。已被Interspeech2023接收</br>
    <b>语音翻译多任务学习</b>                       2022.6-2023.6 </br>
    多任务学习在语音翻译系统的探索。Under review. </br>
    <b>IWSLT22评测任务</b>                           2022.2-2022.5 </br>
    参与IWSLT2022 离线英中端到端语音翻译比赛，主要运用了SATE 和 MSP-ST方法。 </br>
    <b>语音翻译预训练</b>                          2021.3-2021.11 </br>
    探索如有利用无标注的文本和语音数据及有标注的语音识别和翻译数据构建端到端语音翻译系统。论文在腾讯北京信息安全部门实习期间完成，已被AAAI2023接收。 </br>
    <b>小设备模型运算库支持</b>                    2020.11-2020.12 </br>
    将NiuTensor计算应用到ARM架构GPU上，利用ARM computing library中的OpenCL 接口改写NiuTensor中的操作，并在Mali架构的芯片上运行。 </br>
    <b>多语言模型迁移量化分析</b>                    2020.6-2021.3 </br>
    在多语言模型中量化不同语言迁移情况并采取策略优化。Under review. </br>
    <b>WMT20评测任务</b>                            2020.3 – 2020.5 </br>
    带队参加三个富资源任务（英日，日英，英中），以及两个稀缺资源任务（泰米尔语到英语，因纽特到英语）。其中英日，日英获得自动评价第一名，英日，因纽特语获得人工评价第一名。主要运用的技术为:多语言模型，大容量模型，迭代finetune策略，Top-p采样生成伪数据生成策略，利用语言模型进行领域适应等。 </br>
    <b>结构搜索</b>                                2019.10-2019.12 </br>
    将神经结构搜索应用到神经机器翻译上。具体为：利用可微分的方法在语言模型上预训练好一个CELL内部和CELL之间的结构，然后将这个结构运用到神经机器翻译任务上，最后IWSLT英语到越南语任务上获得性能提升。 </br>
    <b>神经机器翻译解码加速</b>                       2019.6-2019.8 </br>
    对于Transformer模型中解码计算过程中解码端的注意力计算进行加速，具体为，利用模型中每一层的注意力分布计算其信息熵作为每一层所含其信息量，发现各层信息量不同但是参数量却一致，然后设计压缩比例对解码端注意力计算的参数进行压缩，在不影响性能的前提下，提升约10%的解码速度。此项工作发表于CCMT2019，并获得最佳中文论文提名。</br>
    <b>WMT19评测任务</b>                             2019.2-2019.4 </br>
    负责古吉拉特与到英语的翻译任务，利用迁移学习，语言语法特点，多种回译方法，ensemble搜索策略，多特征重排序，模型上利用DLCL深层网络。获得自动评价，人工评价第一名。 </br>
    <b>NiuTensor深度学习开源底层计算库维护</b>         2018.1至今 </br>
    1.针对于底层的CUDA计算更新维护。
    2.多卡训练支持。
    3.小设备计算支持。
    4.神经机器翻译运算库加速，设计针对于GPU的并行计算算法以及针对不同张量设计不同存取的算法以及指令集优化。
    项目地址：<a href="https://github.com/NiuTrans/NiuTensor">https://github.com/NiuTrans/NiuTensor</a>
</div>

<div>
<h2>论文成果</h2>
1. Improving End-to-end Speech Translation by Leveraging Auxiliary Speech and Text Data. <b>Yuhao Zhang</b>, Chen Xu, Bojie Hu, Chunliang Zhang, Tong Xiao, Jingbo Zhu. AAAI2023. </br>
2. Information Magnitude Based Dynamic Sub-sampling for Speech-to-text. <b>Yuhao Zhang</b>, Chenghao Gao, Kaiqi Kou, Chen Xu, Tong Xiao, Jingbo Zhu. Interspeech2023. </br>
3. The NiuTrans’s Submission to the IWSLT22 English-to-Chinese Offline Speech Translation Task. <b>Yuhao Zhang</b>, Canan Huang, Chen Xu, Xiaoqian Liu, Bei Li, Anxiang Ma, Tong Xiao, Jingbo Zhu. IWSLT2022. </br>
4. The NiuTrans Machine Translation Systems for WMT20. <b>Yuhao Zhang</b>, Ziyang Wang, Runzhe Cao.et.al. WMT2020. </br>
5. 从粗粒度到细粒度神经机器翻译推断加速方法研究。<b>张裕浩</b>，许诺，李垠桥，肖桐，朱靖波。CCMT2019. </br>
6. Bridging the Granularity Gap for Acoustic Modeling. Chen Xu, <b>Yuhao Zhang</b>, Chengbo Jiao, Xiaoqian Liu, Chi Hu, Xin Zeng, Tong Xiao, Anxiang Ma, Huizhen Wang, JingBo Zhu. ACL2023 findings.</br>
7. 基于多尺度建模的端到端自动语音识别方法。陈昊，张润来，<b>张裕浩</b>，高成浩，许晨，马安香，肖桐，朱靖波。CCL2023. </br>
8. Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders. Chen Xu, Bojie Hu, Yanyang Li, <b>Yuhao Zhang</b>, Shen Huang, Qi Ju, Tong Xiao, Jingbo Zhu. ACL2021. </br>
9. Learning Architectures from an Extended Search Space for Language Modeling. Yinqiao Li, Chi Hu, <b>Yuhao Zhang</b>, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li. ACL2020. </br>
10. CTC-based Non-autoregressive Speech Translation. Chen Xu, Xiaoqian Liu, Xiaowen Liu, Qingxuan Sun, <b>Yuhao Zhang</b>, Murun Yang, Qianqian Dong, Tom Ko, Mingxuan Wang, Tong Xiao, Anxiang Ma, Jingbo Zhu. ACL2023</br>
<a href="https://scholar.google.com/citations?hl=zh-CN&user=p3Om2OcAAAAJ">Google scholar</a>
</div>

<div>
<h2>获奖概览</h2>
<ul>
    <li>WMT19 古吉拉特语到英语 自动评价第一名，人工评价第一名</li>
    <li>WMT20英语到日语 自动评价第一名，人工评价第一名 </li>
    <li>WMT20日语到英语 自动评价第一名，人工评价第二名</li>
    <li>WMT20因纽特到英语 自动评价第三名，人工评价第一名</li>
    <li>IWSLT22英语到中文离线语音翻译 自动评价第三名</li>
    <li>CCMT2019最佳中文论文提名</li>
    <li>2018年计算机学院优秀毕业设计（排名第一）</li>
</ul>

</div>
</body>
</html>