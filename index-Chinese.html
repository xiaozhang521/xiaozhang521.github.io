<!DOCTYPE html>
<html>
<head>
    <title>个人介绍</title>
    <style>
        div {
        margin-left: 15%;
        margin-right: 15%;
        /*width: 210px;
        height: 130px;
         background:green; */
        border-radius: 13px;
        }
        .sample {
        background-color: green;
        margin-left: 3%;
        }
        .sample1 {
        background-color: yelllow;
        }
        .sample2 {
        background-color: solid green;
        margin-left: -13%;
        }
        .photo{
            /*width:120px;
            height: auto;*/
            margin-right: 20%;
            float: right;
            text-align: right;
        }
        a {
          outline: none;
          text-decoration: none;
          padding: 2px 1px 0;
        }
        
        a:link {
          color: #265301;
        }
        
        a:visited {
          color: #437a16;
        }
        
        a:focus {
          border-bottom: 1px solid;
          background: #bae498;
        }
        
        a:hover {
          border-bottom: 1px solid;
          background: #cdfeaa;
        }
        
        a:active {
          background: #265301;
          color: #cdfeaa;
        }
        </style>
</head>
<body>
    <div class="photo">
        <img src="./cv.jpg" alt="Personal photo", width=120px,  height=auto>
    </div>
<div>
    <h1>张裕浩</h1>
    <h2>自然语言处理实验室 东北大学 沈阳</h2>
    <p>机器翻译方向 博士四年级 &nbsp;&nbsp;&nbsp;&nbsp; <a href="https://scholar.google.com/citations?hl=zh-CN&user=p3Om2OcAAAAJ">Google scholar</a> </br> Email: yoohao.zhang@gmail.com</p>
    <p>主要研究方向：<b>1.语音翻译，2.多任务学习，3.多语言神经机器翻译，4.模型加速。</b> 模型加速主要分为针对于底层张量并行计算库进行加速以及模型去冗余计算两个方向；多语言神经翻译方向主要利用一个模型翻译多个任务。目前已将论文及系统运用在IWLST，WMT等多个评测任务中并取得优异成绩。</p>
    <p>曾多次担任NeurIPS、ACL、AAAI、EMNLP、ICASSP等会议的评审。</p>
    <h2>教育经历</h2>
    <dl>
        <dt>2020.9 至今 </dt>
        <dd>计算机科学与技术 博士研究生 东北大学 </dd>
        <dd>东北大学自然语言处理实验室 导师：肖桐教授，朱靖波教授</dd>
        <dt>2018.9-2020.7 </dt>
        <dd>计算机软件与理论 硕士研究生 东北大学</dd>
        <dd>东北大学自然语言处理实验室 导师：肖桐教授</dd>
        <dt>2014.9-2018.7  </dt>
        <dd>计算机科学与技术 本科 东北大学</dd>
      </dl>
 
</div>

<div>
    <h2>论文成果</h2>
    1. <a href="https://arxiv.org/pdf/2312.10952.pdf", target="_blank">Soft Alignment of Modality Space for End-to-end Speech Translation.</a> <b>Yuhao Zhang</b>, Kaiqi Kou, Bei Li, Chen Xu, Chunliang Zhang, Tong Xiao, Jingbo Zhu. <b>ICASSP2024</b>. </br>
    2. <a href="https://arxiv.org/pdf/2311.03810.pdf",target="_blank">Rethinking and Improving Multi-task Learning for End-to-end Speech Translation.</a> <b>Yuhao Zhang</b>, Chen Xu, Bei Li, Hao Chen, Tong Xiao, Chunliang Zhang, and Jingbo Zhu. <b>EMNLP2023</b>. </br>
    3. <a href="https://arxiv.org/pdf/2212.01778.pdf",target="_blank">Improving End-to-end Speech Translation by Leveraging Auxiliary Speech and Text Data.</a> <b>Yuhao Zhang</b>, Chen Xu, Bojie Hu, Chunliang Zhang, Tong Xiao, Jingbo Zhu. <b>AAAI2023</b>. </br>
    4. <a href="https://www.isca-speech.org/archive/pdfs/interspeech_2023/zhang23u_interspeech.pdf",target="_blank">Information Magnitude Based Dynamic Sub-sampling for Speech-to-text.</a> <b>Yuhao Zhang</b>, Chenghao Gao, Kaiqi Kou, Chen Xu, Tong Xiao, Jingbo Zhu. <b>INTERSPEECH2023</b>. </br>
    5. <a href="https://aclanthology.org/2022.iwslt-1.19.pdf",target="_blank">The NiuTrans’s Submission to the IWSLT22 English-to-Chinese Offline Speech Translation Task.</a> <b>Yuhao Zhang</b>, Canan Huang, Chen Xu, Xiaoqian Liu, Bei Li, Anxiang Ma, Tong Xiao, Jingbo Zhu. <b>IWSLT2022</b>. </br>
    6. <a href="https://aclanthology.org/2020.wmt-1.37.pdf",target="_blank">The NiuTrans Machine Translation Systems for WMT20.</a> <b>Yuhao Zhang</b>, Ziyang Wang, Runzhe Cao.et.al. <b>WMT2020</b>. </br>
    7. <a href="https://jxmu.xmu.edu.cn/#/digest?ArticleID=4390",target="_blank">从粗粒度到细粒度神经机器翻译推断加速方法研究。</a> <b>张裕浩</b>，许诺，李垠桥，肖桐，朱靖波。<b>CCMT2019</b>. </br>
    8. <a href="https://arxiv.org/pdf/2309.12234.pdf",target="_blank">Bridging the Gaps of Both Modality and Language: Synchronous Bilingual CTC for Speech Translation and Speech Recognition.</a> Chen Xu, Xiaoqian Liu, Erfeng He, <b>Yuhao Zhang</b>, Qianqian Dong, Tong Xiao, Jingbo Zhu, Dapeng Man, Wu Yang. <b>ICASSP2024</b>. </br>
    9. <a href="https://arxiv.org/pdf/2305.17356.pdf",target="_blank">Bridging the Granularity Gap for Acoustic Modeling.</a> Chen Xu, <b>Yuhao Zhang</b>, Chengbo Jiao, Xiaoqian Liu, Chi Hu, Xin Zeng, Tong Xiao, Anxiang Ma, Huizhen Wang, JingBo Zhu. <b>ACL2023</b> findings.</br>
    10. <a href="https://aclanthology.org/2023.ccl-1.41.pdf",target="_blank">基于多尺度建模的端到端自动语音识别方法。</a> 陈昊，张润来，<b>张裕浩</b>，高成浩，许晨，马安香，肖桐，朱靖波。<b>CCL2023</b>. </br>
    11. <a href="https://arxiv.org/pdf/2105.05752.pdf",target="_blank">Stacked Acoustic-and-Textual Encoding: Integrating the Pre-trained Models into Speech Translation Encoders.</a> Chen Xu, Bojie Hu, Yanyang Li, <b>Yuhao Zhang</b>, Shen Huang, Qi Ju, Tong Xiao, Jingbo Zhu. <b>ACL2021</b>. </br>
    12. <a href="https://aclanthology.org/2020.acl-main.592.pdf",target="_blank">Learning Architectures from an Extended Search Space for Language Modeling.</a> Yinqiao Li, Chi Hu, <b>Yuhao Zhang</b>, Nuo Xu, Yufan Jiang, Tong Xiao, Jingbo Zhu, Tongran Liu, Changliang Li. <b>ACL2020</b>. </br>
    13. <a href="https://aclanthology.org/2023.acl-long.744.pdf",target="_blank">CTC-based Non-autoregressive Speech Translation.</a> Chen Xu, Xiaoqian Liu, Xiaowen Liu, Qingxuan Sun, <b>Yuhao Zhang</b>, Murun Yang, Qianqian Dong, Tom Ko, Mingxuan Wang, Tong Xiao, Anxiang Ma, Jingbo Zhu. <b>ACL2023</b>.</br>
    14. <a href="https://aclanthology.org/2023.iwslt-1.17.pdf",target="_blank">The NiuTrans End-to-End Speech Translation System for IWSLT23 English-to-Chinese Offline Task.</a> Yuchen Han, Xiaoqian Liu, Hao Chen, <b>Yuhao Zhang</b>, Chen Xu, Tong Xiao, Jingbo Zhu. <b>IWSLT2023</b>.</br>
    15. <a href="https://aclanthology.org/W19-5325.pdf",target="_blank">The NiuTrans machine translation systems for WMT19.</a> Bei Li, Yinqiao Li, Chen Xu, Ye Lin, Jiqiang Liu, Hui Liu, Ziyang Wang, <b>Yuhao Zhang</b>, et al. <b>WMT2019</b>.</br>
</div>
    </div>
<div>
    <h2>学习经历 </h2>
    <table>
    <tr><td><b>面向语音翻译的软对齐方法</b> (ICASSP2024)</td>   <td>2023.05-2023.09 </td></tr>
    <tr><td>使用软对齐方式解决模态差异问题。 </td></tr>

    <tr><td><b>面向语音识别的多尺度建模</b> (CCL2023)</td>   <td>2023.02-2023.05 </td></tr>
    <tr><td>使用多尺度建模方式增强端到端语音识别编码，进而提升性能。 </td></tr>
    
    <tr><td><b>语音翻译评测任务</b> (IWSLT2023)</td>        <td>2023.02-2023.04</td> </tr>
    <tr><td>参与IWSLT2023 离线英中端到端语音翻译比赛，主要运用了多任务学习策略、多轮伪数据方法和基于预训练的音频切分方法。 </td></tr>

    <tr><td><b>动态语音特征压缩</b> (Interspeech2023) </td> <td>   2022.05-2023.01</td> </tr>
    <tr><td> 语音前端的特征压缩方法探索。</td></tr>

    <tr><td><b>语音翻译多任务学习</b> (EMNLP2023)  </td>                   <td>2022.06-2023.06 </td></tr>
    <tr><td>多任务学习在语音翻译系统的探索。</td></tr>

    <tr><td><b>语音翻译评测任务</b> (IWSLT2022) </td>                        <td>   2022.02-2022.05 </td></tr>
    <tr><td>参与IWSLT2022 离线英中端到端语音翻译比赛，主要运用了SATE 和 MSP-ST方法。 </td></tr>

    <tr><td><b>语音翻译预训练</b> (AAAI2023) </td>                      <td>  2021.03-2021.11 </td></tr>
    <tr><td>探索如有利用无标注的文本和语音数据及有标注的语音识别和翻译数据构建端到端语音翻译系统。论文主要在腾讯北京信息安全部门实习期间完成。 </td></tr>

    <tr><td><b>小设备模型运算库支持</b></td>                             <td> 2020.11-2020.12 </td></tr>
    <tr><td>将NiuTensor计算应用到ARM架构GPU上，利用ARM computing library中的OpenCL 接口改写NiuTensor中的操作，并在Mali架构的芯片上运行。 </td></tr>

    <tr><td><b>多语言模型迁移量化分析</b> (Under review) </td>           <td> 2020.06-2021.03 </td></tr>
    <tr><td>在多语言模型中量化不同语言迁移情况并采取策略优化。</td> </tr>

    <tr><td><b>机器翻译评测任务</b> (WMT2020)</td>                          <td>2020.03-2020.05 </td></tr>
    <tr><td>带队参加三个富资源任务（英日，日英，英中），以及两个稀缺资源任务（泰米尔语到英语，因纽特到英语）。其中英日，日英获得自动评价第一名，英日，因纽特语获得人工评价第一名。</br>
        主要运用的技术为:多语言模型，大容量模型，迭代finetune策略，Top-p采样生成伪数据生成策略，利用语言模型进行领域适应等。 </td></tr>

    <tr><td><b>结构搜索</b> (ACL2020)</td>                                <td>2019.10-2019.12 </td></tr>
    <tr><td>将神经结构搜索应用到神经机器翻译上。具体为：利用可微分的方法在语言模型上预训练好一个CELL内部和CELL之间的结构，然后将结构运用到神经机器翻译任务上。</br>
        主要在IWSLT英语到越南语任务上获得性能提升。 </td></tr>

    <tr><td><b>神经机器翻译解码加速</b> (CCMT2019) </td>                   <td>2019.06-2019.08 </td></tr>
    <tr><td>对于Transformer模型中解码计算过程中解码端的注意力计算进行加速，具体为，利用模型中每一层的注意力分布计算其信息熵作为其所含信息量，发现各层信息量差异较大。</br>
        根据这种现象设计压缩比例对解码端注意力计算的参数进行压缩，在不影响性能的前提下，提升约10%的解码速度。</td></tr>
    
    <tr><td><b>机器翻译评测任务</b> (WMT2019)</td>                         <td>2019.02-2019.04 </td></tr>
    <tr><td>负责古吉拉特与到英语的翻译任务，利用迁移学习，语言语法特点，多种回译方法，ensemble搜索策略，多特征重排序，模型上利用DLCL深层网络。获得自动评价，人工评价第一名。</td></tr>

    <tr><td><b>NiuTensor深度学习开源底层计算库</b></td>                    <td>2018.1-至今 </td></tr>
    <tr><td>1.针对于底层的CUDA计算更新维护。2.多卡训练支持。3.小设备计算支持。</br>
    4.神经机器翻译运算库加速，设计针对于GPU的并行计算算法以及针对不同张量设计不同存取的算法以及指令集优化。</td></tr>
    <tr><td>项目地址：<a href="https://github.com/NiuTrans/NiuTensor">https://github.com/NiuTrans/NiuTensor</a></td></tr>
    </table>   
</div>

<div>
<h2>获奖概览</h2>
<ul>
    <li>WMT19 古吉拉特语到英语 自动评价第一名，人工评价第一名</li>
    <li>WMT20英语到日语 自动评价第一名，人工评价第一名 </li>
    <li>WMT20日语到英语 自动评价第一名，人工评价第二名</li>
    <li>WMT20因纽特到英语 自动评价第三名，人工评价第一名</li>
    <li>IWSLT22英语到中文离线语音翻译 自动评价第三名</li>
    <li>IWSLT23英语到中文端到端受限离线语音翻译 自动评价第一名</li>
    <li>东北大学优秀博士研究生</li>
    <li>苏州育才博士生奖学金</li>
    <li>CCMT2019最佳中文论文提名</li>
    <li>CCL2023最佳海报展示奖</li>
    <li>2018年计算机学院优秀毕业设计（排名第一）</li>
</ul>

</div>
<p align="center"><a href="./index.html">English</a> </p>
</body>
</html>